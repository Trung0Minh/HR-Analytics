I have a project structure with:
- `src/data_processing.py` - contains data processing functions
- `src/visualization.py` - contains visualization functions  
- `notebooks/01_data_exploration.ipynb` - imports and uses functions from the above files (NO function definitions in notebook)

## Current Dataset Context
I'm analyzing HR Analytics data to predict if a programmer will change jobs. The dataset has these columns:
- `enrollee_id`, `city`, `city_development_index` (float)
- `gender`, `relevent_experience`, `enrolled_university`, `education_level`, `major_discipline`
- `experience` (ordinal: <1, 1, 2, ..., >20), `company_size` (ordinal), `company_type`, `last_new_job` (ordinal: never, 1, 2, 3, 4, >4)
- `training_hours` (int), `target` (0 or 1)

## What I've Already Done
Missing value analysis with heatmap
Target distribution and imbalance check
Analysis of: gender, relevent_experience, enrolled_university, education_level
Numerical features: city_development_index, training_hours
Basic correlation with target
One feature interaction heatmap (experience vs education_level)

## What Needs to Be Added

### 1. Missing Categorical Features Analysis
Add analysis for these features I haven't covered yet:
- `experience` (ordinal categorical)
- `company_size` (ordinal categorical)
- `company_type` (nominal categorical)
- `last_new_job` (ordinal categorical)
- `major_discipline` (need target rate analysis)

### 2. City Analysis
- Number of unique cities
- Top 10 cities by candidate count
- Target rate by city or city groups
- Relationship between `city` and `city_development_index`

### 3. Deep Missing Pattern Analysis
- Correlation between missing values across different columns
- Target rate comparison: records with missing vs without missing for each column
- Multi-column missing patterns

### 4. Additional Feature Interactions
Create interaction analyses for:
- `relevent_experience` vs `experience` (years)
- `company_size` vs `company_type`
- `education_level` vs `major_discipline`
- `last_new_job` vs `target` (important!)
- `enrolled_university` vs `experience`

### 5. Enhanced Outlier Analysis
- Outlier detection for `city_development_index`
- Impact of outliers on target variable

### 6. Data Quality Checks
- Check for duplicate `enrollee_id`
- Identify inconsistent categories
- Logic validation (e.g., experience="<1" but large company_size)

### 7. Segmentation Analysis
- Profile comparison: target=1 vs target=0
- Identify high-risk segments (high target rate)

### 8. Statistical Significance Tests
- Chi-square tests for categorical features vs target
- Mann-Whitney U test for numerical features vs target
- Feature importance ranking

### 9. Enhanced Business Insights
- Why do lower city_development_index correlate with higher target rate?
- Interpretation of "no relevent experience" with many years of experience
- Deep dive into features with unexpected patterns

## Requirements

**For `src/data_processing.py`**, add new necessary functions.

**For `src/visualization.py`**, add new necessary functions.

**For `notebooks/01_data_exploration.ipynb`**, add new sections:
- Section 5.5: onwards Analysis of remaining categorical features (experience, company_size, company_type, last_new_job, major_discipline target rate)
- Section 6.2: Deep missing pattern analysis
- Section 6.3: Additional feature interactions (5 new interactions)
- Section 7: City analysis
- Section 8: Outlier analysis enhancement
- Section 9: Data quality validation
- Section 10: Segmentation and profiling
- Section 11: Statistical significance testing
- Section 12: Enhanced conclusions with business insights

## Important Notes
- Use NumPy structured arrays (as shown in my current code)
- All functions should be in `.py` files, notebook only imports and calls them
- Add proper docstrings to all new functions
- Include error handling
- Follow the existing code style
- Visualizations should be clear and informative
- Add detailed comments explaining the insights
- Can only use NumPy, Matplotlib, Seaborn.

Make sure all code is production-ready, well-documented, and follows best practices.